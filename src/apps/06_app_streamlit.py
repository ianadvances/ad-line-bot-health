import os
import streamlit as st
from dotenv import load_dotenv
from FlagEmbedding import BGEM3FlagModel
import chromadb
from openai import OpenAI
from typing import List, Dict
import sys
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ Python è·¯å¾‘ä»¥ä¾›åŒ¯å…¥
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.utils import PathHelper, get_logger

# å¸¸æ•¸å®šç¾©
CHANNEL_NAME = 'Cofit211'
CHROMA_DB = 'Cofit211-cosine'
CHAT_MODEL_NAME = "gpt-4o-mini"
INIT_MESSAGE = "æ‚¨å¥½!æˆ‘æ˜¯æ‚¨çš„ AI è«®è©¢å¸«ã€‚æœ‰ä»€éº¼æˆ‘å¯ä»¥å¹«æ‚¨çš„å—?"

# è¨­å®šæ—¥èªŒ
logger = get_logger(__name__)

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
dotenv_path = PathHelper.root_dir / ".env"
load_dotenv(dotenv_path=dotenv_path)

st.set_page_config(page_title="AI ä¿å¥å®¤", page_icon="ğŸ§‘â€âš•ï¸")
st.title("ğŸ‘¨ğŸ»â€âš•ï¸ AI ä¿å¥å®¤ ğŸ§‘ğŸ»â€âš•ï¸")

# è¨­å®š OpenAI
client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))

@st.cache_resource
def get_embedding_model():
    return BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)

@st.cache_resource(ttl=24 * 3600)
def configure_retriever():
    client = chromadb.PersistentClient(path=str(PathHelper.db_dir))
    collection = client.get_collection(name=CHROMA_DB)
    return collection

def get_relevant_documents(query: str, collection, model, top_k: int = 3) -> List[Dict]:
    query_embedding = model.encode(query, batch_size=1, max_length=8192)['dense_vecs']
    results = collection.query(
        query_embeddings=[query_embedding.tolist()],
        n_results=top_k,
        include=["documents", "metadatas"]
    )
    documents = [
        {"page_content": doc, "metadata": meta} 
        for doc, meta in zip(results['documents'][0], results['metadatas'][0])
    ]
    return documents

def generate_response(messages: List[Dict]) -> str:
    try:
        chat_completion = client.chat.completions.create(
            model=CHAT_MODEL_NAME,
            messages=messages,
            temperature=0,
            stream=True
        )
        full_response = ""
        for chunk in chat_completion:
            if chunk.choices[0].delta.content:
                full_response += chunk.choices[0].delta.content
                yield full_response
    except Exception as e:
        logger.error(f"OpenAI API éŒ¯èª¤: {str(e)}")
        yield f"æŠ±æ­‰,ç™¼ç”Ÿäº†ä¸€å€‹éŒ¯èª¤: {str(e)}"

def get_cumulative_query(messages: List[Dict]) -> str:
    return " ".join([msg["content"] for msg in messages if msg["role"] == "user"])

embedding_model = get_embedding_model()
collection = configure_retriever()

# åˆå§‹åŒ–æˆ–å–å¾—èŠå¤©æ­·å²
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": INIT_MESSAGE}]

# é¡¯ç¤ºèŠå¤©æ­·å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ä½¿ç”¨è€…è¼¸å…¥
if user_query := st.chat_input(placeholder="è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ"):
    st.session_state.messages.append({"role": "user", "content": user_query})
    with st.chat_message("user"):
        st.markdown(user_query)

    # å–å¾—ç´¯ç©çš„æŸ¥è©¢
    cumulative_query = get_cumulative_query(st.session_state.messages)
    
    # å–å¾—ç›¸é—œæ–‡ä»¶
    relevant_docs = get_relevant_documents(cumulative_query, collection, embedding_model, top_k=3)
    context = relevant_docs[0]['page_content'] if relevant_docs else ""

    # æº–å‚™å‚³é€çµ¦ AI çš„è¨Šæ¯
    messages_for_ai = [
        {"role": "system", "content": f"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å¥åº·ã€é†«ç™‚å’Œé£²é£Ÿç›¸é—œçš„è«®è©¢å¸«ã€‚ç”¨ç¹é«”ä¸­æ–‡å›è¦†ã€‚å¯ä»¥åƒè€ƒä»¥ä¸‹èƒŒæ™¯è³‡è¨Šä½†ä¸é™æ–¼æ­¤ä¾†å›ç­”å•é¡Œ:\n\n{context}\né™¤äº†ä»¥ä¸Šè³‡è¨Šä½ å¯ä»¥å†é€²è¡Œè£œå……ï¼Œå›ç­”ä¸ç”¨éé•·ï¼Œå›ç­”å¾—æœ‰çµæ§‹åŠå®Œæ•´å°±å¥½ã€‚å¿…è¦æ™‚å¯ä»¥è·Ÿä½¿ç”¨è€…è©¢å•æ›´å¤šè³‡è¨Šä¾†æä¾›çµ¦ä½ ã€‚"},
    ] + st.session_state.messages[-5:]  # åªå‚³é€æœ€å¾Œ5æ¢è¨Šæ¯ä»¥ç¯€çœ tokens

    # ç”¢ç”Ÿå›æ‡‰
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        for response in generate_response(messages_for_ai):
            response_placeholder.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})

    # é¡¯ç¤ºç›¸é—œå½±ç‰‡çš„é€£çµ
    if relevant_docs:
        st.write("ç›¸é—œå½±ç‰‡:")
        for i, doc in enumerate(relevant_docs, 1):
            video_id = doc['metadata']['video_id']
            youtube_url = f"https://www.youtube.com/watch?v={video_id}"
            st.write(f"{i}. [{youtube_url}]({youtube_url})")

    # åœ¨å°è©±æ¡†å¤–åŠ å…¥æª¢è¦–ç›¸é—œå½±ç‰‡çš„æŒ‰éˆ•
    for i, doc in enumerate(relevant_docs, 1):
        video_id = doc['metadata']['video_id']
        youtube_url = f"https://www.youtube.com/watch?v={video_id}"
        if st.button(f"æª¢è¦–ç›¸é—œå½±ç‰‡ {i}"):
            st.video(youtube_url)

# å´é‚Šæ¬„
st.sidebar.title("è¨­å®š")
if st.sidebar.button("æ¸…é™¤å°è©±æ­·å²"):
    st.session_state.messages = [{"role": "assistant", "content": INIT_MESSAGE}]
st.sidebar.info("é€™æ˜¯ä¸€å€‹åŸºæ–¼ AI çš„å¥åº·è«®è©¢å¸«")

if __name__ == "__main__":
    pass